{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing step\n",
    "\n",
    "I first need to do the data cleaning the details of steps are below :\n",
    "\n",
    "(1)Load the json and csv file - with pd.read_json and pd.read_csv\n",
    "\n",
    "(2)Combine the Dataset - with pd.merge\n",
    "\n",
    "(3)Text cleaning - with cleanTxt\n",
    "\n",
    "(4)Create the dataset  - with to_pickle\n",
    "\n",
    "(5)Split the training dataset into train and validation set - with train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the train.csv and test.csv\n",
    "df =pd.read_csv('data_identification.csv')\n",
    "train_df = df.loc[df['identification']=='train']\n",
    "test_df = df.loc[df['identification']=='test']\n",
    "\n",
    "header = ['tweet_id', 'identification']\n",
    "train_df.to_csv('train.csv', index = False, columns = header)\n",
    "test_df.to_csv('test.csv', index = False, columns = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1455563 411972\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  tweet_id  \\\n",
       "0                     [Snapchat]  0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                   [bibleverse]  0x28b412   \n",
       "3                             []  0x1cd5b0   \n",
       "4                             []  0x2de201   \n",
       "\n",
       "                                                text  \n",
       "0  People who post \"add me on #Snapchat\" must be ...  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  \n",
       "2  Confident of your obedience, I write to you, k...  \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
       "4  \"Trust is not the same as faith. A friend is s...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the json file \n",
    "try_df = pd.read_json('tweets_DM.json', lines=True)\n",
    "\n",
    "input_dict = list(pd.DataFrame(list(try_df._source.values)).tweet)\n",
    "df_dict = pd.DataFrame.from_dict(input_dict)\n",
    "df_dict.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLean the text\n",
    "import re\n",
    "def cleanTxt(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text)\n",
    "    text = re.sub(r'RT[\\s]+', '', text)\n",
    "    text = re.sub(r'http?:\\/\\/\\S+','',text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on Snapchat\" must be d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>As we see, Trump is dangerous to freepress ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  tweet_id  \\\n",
       "0                     [Snapchat]  0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                   [bibleverse]  0x28b412   \n",
       "3                             []  0x1cd5b0   \n",
       "4                             []  0x2de201   \n",
       "\n",
       "                                                text  \n",
       "0  People who post \"add me on Snapchat\" must be d...  \n",
       "1   As we see, Trump is dangerous to freepress ar...  \n",
       "2  Confident of your obedience, I write to you, k...  \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
       "4  \"Trust is not the same as faith. A friend is s...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning the text\n",
    "df_dict['text'] = df_dict['text'].apply(cleanTxt)\n",
    "df_dict.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_json(\"tweets_DM.json\", encoding=\\'utf-8\\', lines=True)\\ndf_label = pd.read_csv(\"emotion.csv\")\\n\\nSeries_of_dict = list(pd.DataFrame(list(df._source.values)).tweet)\\ndf_data = pd.DataFrame.from_dict(Series_of_dict)\\ndf_data.head()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df = pd.read_json(\"tweets_DM.json\", encoding='utf-8', lines=True)\n",
    "df_label = pd.read_csv(\"emotion.csv\")\n",
    "\n",
    "Series_of_dict = list(pd.DataFrame(list(df._source.values)).tweet)\n",
    "df_data = pd.DataFrame.from_dict(Series_of_dict)\n",
    "df_data.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>Huge Respect🖒  talking about losing his dad to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>[spateradio, app]</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>_BCH   Well done team 🌟 &lt;LH&gt; of every one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>train</td>\n",
       "      <td>[PUBG, GamersUnite, twitch, BeHealthy, StayPos...</td>\n",
       "      <td>Come join  on PUBG while he strives for chicke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>train</td>\n",
       "      <td>[strength, bones, God]</td>\n",
       "      <td>Blessings!My strength little. My bones brittl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification                                           hashtags  \\\n",
       "0  0x29e452          train                                                 []   \n",
       "1  0x2b3819          train                                  [spateradio, app]   \n",
       "2  0x2a2acc          train                                                 []   \n",
       "3  0x2a8830          train  [PUBG, GamersUnite, twitch, BeHealthy, StayPos...   \n",
       "4  0x20b21d          train                             [strength, bones, God]   \n",
       "\n",
       "                                                text  \n",
       "0  Huge Respect🖒  talking about losing his dad to...  \n",
       "1  Yoooo we hit all our monthly goals with the ne...  \n",
       "2   _BCH   Well done team 🌟 <LH> of every one of ...  \n",
       "3  Come join  on PUBG while he strives for chicke...  \n",
       "4   Blessings!My strength little. My bones brittl...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merg_df = pd.merge(train_df, df_dict, how='left', on='tweet_id')\n",
    "train_merg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>Huge Respect🖒  talking about losing his dad to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text\n",
       "0  0x29e452  Huge Respect🖒  talking about losing his dad to...\n",
       "1  0x2b3819  Yoooo we hit all our monthly goals with the ne..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = ['tweet_id', 'identification']\n",
    "train_merg_df[['tweet_id', 'text']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>Huge Respect🖒  talking about losing his dad to...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text emotion\n",
       "0  0x29e452  Huge Respect🖒  talking about losing his dad to...     joy\n",
       "1  0x2b3819  Yoooo we hit all our monthly goals with the ne...     joy"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df=pd.read_csv('emotion.csv')\n",
    "train_merg_df2 = pd.merge(train_merg_df, emotion_df, how='left', on='tweet_id')\n",
    "train_merg_df2[['tweet_id', 'text','emotion']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>I've seen two separate colours of the elegant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>No serious self respecting individual believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>test</td>\n",
       "      <td>[womendrivers]</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>test</td>\n",
       "      <td>[robbingmembers]</td>\n",
       "      <td>“only the brave” just out and fountain park h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>Felt like total dog 💩 going into open gym and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification          hashtags  \\\n",
       "0  0x28cc61           test                []   \n",
       "1  0x2db41f           test                []   \n",
       "2  0x2466f6           test    [womendrivers]   \n",
       "3  0x23f9e9           test  [robbingmembers]   \n",
       "4  0x1fb4e1           test                []   \n",
       "\n",
       "                                                text  \n",
       "0   I've seen two separate colours of the elegant...  \n",
       "1    No serious self respecting individual believ...  \n",
       "2  Looking for a new car, and it says 1 lady owne...  \n",
       "3   “only the brave” just out and fountain park h...  \n",
       "4  Felt like total dog 💩 going into open gym and ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merg_df = pd.merge(test_df, df_dict, how='left', on='tweet_id')\n",
    "test_merg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455563, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_emotion = train_merg_df2[['tweet_id', 'text','emotion']]\n",
    "train_with_emotion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411972, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_with_emotion = test_merg_df[['tweet_id', 'text']]\n",
    "test_with_emotion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_emotion.to_pickle(\"train_with_emotion.pkl\") \n",
    "train_with_emotion_df = pd.read_pickle(\"train_with_emotion.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_emotion.to_pickle(\"test_with_emotion.pkl\")\n",
    "test_with_emotion_df = pd.read_pickle(\"test_with_emotion.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the training dataset into train and val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>754714</th>\n",
       "      <td>0x2f11c4</td>\n",
       "      <td>FailureFriday ....Now , Back to Russia! Impeac...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105795</th>\n",
       "      <td>0x3072b5</td>\n",
       "      <td>&lt;LH&gt; Holidays</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33277</th>\n",
       "      <td>0x1cc8b3</td>\n",
       "      <td>Tonight  &lt;LH&gt;  8pm-12MidNite  Victory &lt;LH&gt; Chapel</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723943</th>\n",
       "      <td>0x30c714</td>\n",
       "      <td>Sitting on a plane reflecting on the many lead...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139949</th>\n",
       "      <td>0x2b45aa</td>\n",
       "      <td>Only sikhs have conquered Afghanistan. Many s...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_id                                               text  \\\n",
       "754714  0x2f11c4  FailureFriday ....Now , Back to Russia! Impeac...   \n",
       "105795  0x3072b5                                      <LH> Holidays   \n",
       "33277   0x1cc8b3  Tonight  <LH>  8pm-12MidNite  Victory <LH> Chapel   \n",
       "723943  0x30c714  Sitting on a plane reflecting on the many lead...   \n",
       "139949  0x2b45aa   Only sikhs have conquered Afghanistan. Many s...   \n",
       "\n",
       "             emotion  \n",
       "754714       sadness  \n",
       "105795           joy  \n",
       "33277   anticipation  \n",
       "723943         trust  \n",
       "139949         trust  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test = train_test_split(train_with_emotion, test_size = 0.3, random_state = 42, shuffle = True)\n",
    "X_train, X_test = train_test_split(train_with_emotion, test_size = 0.2, random_state = 1, shuffle = True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>764928</th>\n",
       "      <td>0x2ed744</td>\n",
       "      <td>please make the coconut shrimp taco a permane...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411174</th>\n",
       "      <td>0x255371</td>\n",
       "      <td>Lesson of the Day: you never know when your ki...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008269</th>\n",
       "      <td>0x1d36fb</td>\n",
       "      <td>What wonderful presentation events we have had...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63685</th>\n",
       "      <td>0x238b4f</td>\n",
       "      <td>&lt;LH&gt; to , graphic design and sign specialists,...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970075</th>\n",
       "      <td>0x262da1</td>\n",
       "      <td>_1 Now there are two idiots!  clueless &lt;LH&gt;</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "764928   0x2ed744   please make the coconut shrimp taco a permane...   \n",
       "1411174  0x255371  Lesson of the Day: you never know when your ki...   \n",
       "1008269  0x1d36fb  What wonderful presentation events we have had...   \n",
       "63685    0x238b4f  <LH> to , graphic design and sign specialists,...   \n",
       "970075   0x262da1        _1 Now there are two idiots!  clueless <LH>   \n",
       "\n",
       "              emotion  \n",
       "764928            joy  \n",
       "1411174         trust  \n",
       "1008269  anticipation  \n",
       "63685             joy  \n",
       "970075        sadness  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## save to pickle file\n",
    "X_train.to_pickle(\"model_X_train.pkl\") \n",
    "X_test.to_pickle(\"model_X_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## load a pickle file\n",
    "model_X_train_df = pd.read_pickle(\"model_X_train.pkl\")\n",
    "model_X_test_df = pd.read_pickle(\"model_X_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "As learn a lot from lab2, I continute use BOW to do with feature engineering\n",
    "\n",
    "But this time I set the feature to 1000 to think it can do better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DM\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1164450, 1000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "BOW_1000 = CountVectorizer(max_features=1000, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "BOW_1000.fit(model_X_train_df['text'])\n",
    "print(\"ok\")\n",
    "BOW_features_1000 = BOW_1000.transform(model_X_train_df['text'])\n",
    "BOW_features_1000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1164450, 1000)\n",
      "y_train.shape:  (1164450,)\n",
      "X_test.shape:  (291113, 1000)\n",
      "y_test.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# standardize name (X, y) \n",
    "X_train = BOW_1000.transform(model_X_train_df['text'])\n",
    "y_train = model_X_train_df['emotion']\n",
    "\n",
    "X_test = BOW_1000.transform(model_X_test_df['text'])\n",
    "y_test = model_X_test_df['emotion']\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with categorical label (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:8]:\n",
      " 754714          sadness\n",
      "105795              joy\n",
      "33277      anticipation\n",
      "723943            trust\n",
      "139949            trust\n",
      "863995          sadness\n",
      "1131056    anticipation\n",
      "835483              joy\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_test.shape:  (291113,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:8]:\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1164450, 8)\n",
      "y_test.shape:  (291113, 8)\n"
     ]
    }
   ],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:8]:\\n', y_train[0:8])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:8]:\\n', y_train[0:8])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  1000\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                64064     \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "softmax_2 (Softmax)          (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 77,064\n",
      "Trainable params: 77,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 1000\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 3rd hidden layer\n",
    "H2_W3 = Dense(units=64)(H2)  # 64\n",
    "H3 = ReLU()(H2_W3)\n",
    "\n",
    "# 4th hidden layer\n",
    "H3_W4 = Dense(units=64)(H3)  # 64\n",
    "H4 = ReLU()(H3_W4)\n",
    "\n",
    "# output layer\n",
    "H5_W6 = Dense(units=output_shape)(H4)  # 4\n",
    "H5 = Softmax()(H5_W6)\n",
    "\n",
    "model_output = H5\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                64064     \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 68,744\n",
      "Trainable params: 68,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "777/777 [==============================] - 8s 10ms/step - loss: 1.4621 - accuracy: 0.4667 - val_loss: 1.3938 - val_accuracy: 0.4897\n",
      "Epoch 2/100\n",
      "777/777 [==============================] - 8s 11ms/step - loss: 1.3747 - accuracy: 0.4965 - val_loss: 1.3728 - val_accuracy: 0.4973\n",
      "Epoch 3/100\n",
      "777/777 [==============================] - 9s 11ms/step - loss: 1.3539 - accuracy: 0.5042 - val_loss: 1.3649 - val_accuracy: 0.4998\n",
      "Epoch 4/100\n",
      "777/777 [==============================] - 9s 11ms/step - loss: 1.3412 - accuracy: 0.5088 - val_loss: 1.3608 - val_accuracy: 0.5022\n",
      "Epoch 5/100\n",
      "777/777 [==============================] - 9s 11ms/step - loss: 1.3325 - accuracy: 0.5121 - val_loss: 1.3566 - val_accuracy: 0.5037\n",
      "Epoch 6/100\n",
      "777/777 [==============================] - 9s 12ms/step - loss: 1.3257 - accuracy: 0.5148 - val_loss: 1.3546 - val_accuracy: 0.5045\n",
      "Epoch 7/100\n",
      "777/777 [==============================] - 9s 12ms/step - loss: 1.3202 - accuracy: 0.5173 - val_loss: 1.3548 - val_accuracy: 0.5043\n",
      "Epoch 8/100\n",
      "777/777 [==============================] - 9s 12ms/step - loss: 1.3154 - accuracy: 0.5193 - val_loss: 1.3548 - val_accuracy: 0.5051\n",
      "Epoch 9/100\n",
      "777/777 [==============================] - 9s 12ms/step - loss: 1.3116 - accuracy: 0.5206 - val_loss: 1.3542 - val_accuracy: 0.5056\n",
      "Epoch 10/100\n",
      "777/777 [==============================] - 10s 12ms/step - loss: 1.3081 - accuracy: 0.5218 - val_loss: 1.3537 - val_accuracy: 0.5059\n",
      "Epoch 11/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.3053 - accuracy: 0.5230 - val_loss: 1.3557 - val_accuracy: 0.5048\n",
      "Epoch 12/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.3026 - accuracy: 0.5241 - val_loss: 1.3549 - val_accuracy: 0.5044\n",
      "Epoch 13/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.3005 - accuracy: 0.5248 - val_loss: 1.3554 - val_accuracy: 0.5052\n",
      "Epoch 14/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2981 - accuracy: 0.5257 - val_loss: 1.3557 - val_accuracy: 0.5044\n",
      "Epoch 15/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2963 - accuracy: 0.5266 - val_loss: 1.3566 - val_accuracy: 0.5043\n",
      "Epoch 16/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2947 - accuracy: 0.5272 - val_loss: 1.3568 - val_accuracy: 0.5054\n",
      "Epoch 17/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2929 - accuracy: 0.5275 - val_loss: 1.3575 - val_accuracy: 0.5038\n",
      "Epoch 18/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2914 - accuracy: 0.5285 - val_loss: 1.3590 - val_accuracy: 0.5037\n",
      "Epoch 19/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2903 - accuracy: 0.5287 - val_loss: 1.3591 - val_accuracy: 0.5042\n",
      "Epoch 20/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2885 - accuracy: 0.5293 - val_loss: 1.3572 - val_accuracy: 0.5049\n",
      "Epoch 21/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2873 - accuracy: 0.5300 - val_loss: 1.3582 - val_accuracy: 0.5046\n",
      "Epoch 22/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2861 - accuracy: 0.5304 - val_loss: 1.3585 - val_accuracy: 0.5035\n",
      "Epoch 23/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2851 - accuracy: 0.5307 - val_loss: 1.3649 - val_accuracy: 0.5037\n",
      "Epoch 24/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2841 - accuracy: 0.5310 - val_loss: 1.3626 - val_accuracy: 0.5038\n",
      "Epoch 25/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2829 - accuracy: 0.5320 - val_loss: 1.3605 - val_accuracy: 0.5044\n",
      "Epoch 26/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2819 - accuracy: 0.5318 - val_loss: 1.3618 - val_accuracy: 0.5033\n",
      "Epoch 27/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2812 - accuracy: 0.5324 - val_loss: 1.3629 - val_accuracy: 0.5035\n",
      "Epoch 28/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2802 - accuracy: 0.5329 - val_loss: 1.3636 - val_accuracy: 0.5028\n",
      "Epoch 29/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2793 - accuracy: 0.5329 - val_loss: 1.3636 - val_accuracy: 0.5029\n",
      "Epoch 30/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2784 - accuracy: 0.5330 - val_loss: 1.3662 - val_accuracy: 0.5019\n",
      "Epoch 31/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2776 - accuracy: 0.5338 - val_loss: 1.3653 - val_accuracy: 0.5022\n",
      "Epoch 32/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2767 - accuracy: 0.5340 - val_loss: 1.3660 - val_accuracy: 0.5028\n",
      "Epoch 33/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2762 - accuracy: 0.5342 - val_loss: 1.3647 - val_accuracy: 0.5031\n",
      "Epoch 34/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2756 - accuracy: 0.5343 - val_loss: 1.3645 - val_accuracy: 0.5030\n",
      "Epoch 35/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2747 - accuracy: 0.5345 - val_loss: 1.3715 - val_accuracy: 0.5020\n",
      "Epoch 36/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2741 - accuracy: 0.5349 - val_loss: 1.3672 - val_accuracy: 0.5029\n",
      "Epoch 37/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2735 - accuracy: 0.5355 - val_loss: 1.3700 - val_accuracy: 0.5008\n",
      "Epoch 38/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2729 - accuracy: 0.5353 - val_loss: 1.3740 - val_accuracy: 0.4994\n",
      "Epoch 39/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2721 - accuracy: 0.5357 - val_loss: 1.3755 - val_accuracy: 0.5004\n",
      "Epoch 40/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2718 - accuracy: 0.5359 - val_loss: 1.3721 - val_accuracy: 0.5006\n",
      "Epoch 41/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2708 - accuracy: 0.5363 - val_loss: 1.3707 - val_accuracy: 0.5019\n",
      "Epoch 42/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2708 - accuracy: 0.5362 - val_loss: 1.3732 - val_accuracy: 0.5014\n",
      "Epoch 43/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2699 - accuracy: 0.5368 - val_loss: 1.3742 - val_accuracy: 0.5028\n",
      "Epoch 44/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2693 - accuracy: 0.5369 - val_loss: 1.3768 - val_accuracy: 0.5011\n",
      "Epoch 45/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2688 - accuracy: 0.5369 - val_loss: 1.3754 - val_accuracy: 0.5002\n",
      "Epoch 46/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2685 - accuracy: 0.5371 - val_loss: 1.3758 - val_accuracy: 0.5021\n",
      "Epoch 47/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2678 - accuracy: 0.5374 - val_loss: 1.3758 - val_accuracy: 0.5021\n",
      "Epoch 48/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2675 - accuracy: 0.5374 - val_loss: 1.3762 - val_accuracy: 0.5009\n",
      "Epoch 49/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2669 - accuracy: 0.5377 - val_loss: 1.3779 - val_accuracy: 0.5011\n",
      "Epoch 50/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2665 - accuracy: 0.5376 - val_loss: 1.3787 - val_accuracy: 0.5006\n",
      "Epoch 51/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2660 - accuracy: 0.5382 - val_loss: 1.3801 - val_accuracy: 0.4999\n",
      "Epoch 52/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2658 - accuracy: 0.5381 - val_loss: 1.3813 - val_accuracy: 0.5014\n",
      "Epoch 53/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2650 - accuracy: 0.5386 - val_loss: 1.3842 - val_accuracy: 0.4981\n",
      "Epoch 54/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2648 - accuracy: 0.5384 - val_loss: 1.3771 - val_accuracy: 0.5013\n",
      "Epoch 55/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2643 - accuracy: 0.5385 - val_loss: 1.3784 - val_accuracy: 0.5014\n",
      "Epoch 56/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2636 - accuracy: 0.5388 - val_loss: 1.3802 - val_accuracy: 0.5015\n",
      "Epoch 57/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2635 - accuracy: 0.5387 - val_loss: 1.3803 - val_accuracy: 0.5003\n",
      "Epoch 58/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2629 - accuracy: 0.5391 - val_loss: 1.3812 - val_accuracy: 0.4994\n",
      "Epoch 59/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2626 - accuracy: 0.5394 - val_loss: 1.3845 - val_accuracy: 0.5004\n",
      "Epoch 60/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2623 - accuracy: 0.5392 - val_loss: 1.3824 - val_accuracy: 0.4997\n",
      "Epoch 61/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2619 - accuracy: 0.5397 - val_loss: 1.3840 - val_accuracy: 0.5003\n",
      "Epoch 62/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2615 - accuracy: 0.5398 - val_loss: 1.3844 - val_accuracy: 0.5003\n",
      "Epoch 63/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2614 - accuracy: 0.5398 - val_loss: 1.3801 - val_accuracy: 0.5000\n",
      "Epoch 64/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2610 - accuracy: 0.5397 - val_loss: 1.3867 - val_accuracy: 0.4996\n",
      "Epoch 65/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2605 - accuracy: 0.5398 - val_loss: 1.3871 - val_accuracy: 0.5001\n",
      "Epoch 66/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2603 - accuracy: 0.5401 - val_loss: 1.3890 - val_accuracy: 0.4986\n",
      "Epoch 67/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2600 - accuracy: 0.5402 - val_loss: 1.3863 - val_accuracy: 0.5004\n",
      "Epoch 68/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2597 - accuracy: 0.5402 - val_loss: 1.3866 - val_accuracy: 0.4997\n",
      "Epoch 69/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2592 - accuracy: 0.5404 - val_loss: 1.3874 - val_accuracy: 0.5005\n",
      "Epoch 70/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2590 - accuracy: 0.5405 - val_loss: 1.3896 - val_accuracy: 0.4986\n",
      "Epoch 71/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2587 - accuracy: 0.5408 - val_loss: 1.3849 - val_accuracy: 0.4996\n",
      "Epoch 72/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2583 - accuracy: 0.5408 - val_loss: 1.3875 - val_accuracy: 0.5005\n",
      "Epoch 73/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2580 - accuracy: 0.5412 - val_loss: 1.3866 - val_accuracy: 0.4998\n",
      "Epoch 74/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2579 - accuracy: 0.5410 - val_loss: 1.3941 - val_accuracy: 0.4991\n",
      "Epoch 75/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2576 - accuracy: 0.5414 - val_loss: 1.3861 - val_accuracy: 0.5003\n",
      "Epoch 76/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2571 - accuracy: 0.5413 - val_loss: 1.3934 - val_accuracy: 0.4980\n",
      "Epoch 77/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2570 - accuracy: 0.5414 - val_loss: 1.3917 - val_accuracy: 0.4986\n",
      "Epoch 78/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2566 - accuracy: 0.5415 - val_loss: 1.3936 - val_accuracy: 0.4992\n",
      "Epoch 79/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2564 - accuracy: 0.5418 - val_loss: 1.3921 - val_accuracy: 0.4982\n",
      "Epoch 80/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2562 - accuracy: 0.5418 - val_loss: 1.3913 - val_accuracy: 0.4995\n",
      "Epoch 81/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2558 - accuracy: 0.5419 - val_loss: 1.3939 - val_accuracy: 0.4989\n",
      "Epoch 82/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2556 - accuracy: 0.5419 - val_loss: 1.3918 - val_accuracy: 0.4995\n",
      "Epoch 83/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2554 - accuracy: 0.5421 - val_loss: 1.3959 - val_accuracy: 0.4977\n",
      "Epoch 84/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2550 - accuracy: 0.5423 - val_loss: 1.3963 - val_accuracy: 0.4983\n",
      "Epoch 85/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2548 - accuracy: 0.5422 - val_loss: 1.3944 - val_accuracy: 0.4987\n",
      "Epoch 86/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2545 - accuracy: 0.5423 - val_loss: 1.3949 - val_accuracy: 0.4983\n",
      "Epoch 87/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2544 - accuracy: 0.5425 - val_loss: 1.3953 - val_accuracy: 0.4984\n",
      "Epoch 88/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2543 - accuracy: 0.5427 - val_loss: 1.3973 - val_accuracy: 0.4989\n",
      "Epoch 89/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2539 - accuracy: 0.5423 - val_loss: 1.3974 - val_accuracy: 0.4979\n",
      "Epoch 90/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2538 - accuracy: 0.5427 - val_loss: 1.3941 - val_accuracy: 0.4978\n",
      "Epoch 91/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2534 - accuracy: 0.5425 - val_loss: 1.3960 - val_accuracy: 0.4984\n",
      "Epoch 92/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2531 - accuracy: 0.5425 - val_loss: 1.4000 - val_accuracy: 0.4981\n",
      "Epoch 93/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2533 - accuracy: 0.5427 - val_loss: 1.3999 - val_accuracy: 0.4976\n",
      "Epoch 94/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2525 - accuracy: 0.5427 - val_loss: 1.4000 - val_accuracy: 0.4986\n",
      "Epoch 95/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2525 - accuracy: 0.5429 - val_loss: 1.3977 - val_accuracy: 0.4975\n",
      "Epoch 96/100\n",
      "777/777 [==============================] - 11s 14ms/step - loss: 1.2524 - accuracy: 0.5428 - val_loss: 1.4020 - val_accuracy: 0.4963\n",
      "Epoch 97/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2522 - accuracy: 0.5434 - val_loss: 1.4040 - val_accuracy: 0.4974\n",
      "Epoch 98/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2518 - accuracy: 0.5432 - val_loss: 1.4014 - val_accuracy: 0.4965\n",
      "Epoch 99/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2518 - accuracy: 0.5432 - val_loss: 1.4004 - val_accuracy: 0.4972\n",
      "Epoch 100/100\n",
      "777/777 [==============================] - 10s 13ms/step - loss: 1.2515 - accuracy: 0.5434 - val_loss: 1.4008 - val_accuracy: 0.4979\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 100\n",
    "batch_size = 1500\n",
    "\n",
    "# training!\n",
    "model.save(\"model_clean_f.h5\")\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "model.save(\"model_clean_e.h5\")\n",
    "\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "M_new_model = load_model(\"model_clean_e.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DM\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(411972, 1000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature engineering on testing set\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "t_BOW_1000 = CountVectorizer(max_features=1000, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "t_BOW_1000.fit(test_with_emotion_df['text'])\n",
    "t_BOW_features_1000 = t_BOW_1000.transform(test_with_emotion_df['text'])\n",
    "print('ok')\n",
    "t_BOW_features_1000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = M_new_model.predict(t_BOW_features_1000, batch_size=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['trust', 'trust', 'trust', 'joy', 'trust'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "sub4 = pred_result.tolist()\n",
    "sub4_df = DataFrame(sub4,columns=['emotion'])\n",
    "sub4_df.to_csv('sub_model_clean_e33.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "t_BOW_1000 = CountVectorizer(max_features=1000, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "t_BOW_1000.fit(test_with_emotion_df['text'])\n",
    "\n",
    "t_BOW_features_1000 = t_BOW_1000.transform(test_with_emotion_df['text'])\n",
    "print('ok')\n",
    "t_BOW_features_1000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = new_model.predict(t_BOW_features_1000, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "t_BOW_1000 = CountVectorizer(max_features=1000, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "t_BOW_1000.fit(test_with_emotion_df['text'])\n",
    "\n",
    "t_BOW_features_1000 = t_BOW_1000.transform(test_with_emotion_df['text'])\n",
    "print('ok')\n",
    "t_BOW_features_1000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = model.predict(t_BOW_features_1000, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pred_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pred_result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('sub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM",
   "language": "python",
   "name": "dm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
